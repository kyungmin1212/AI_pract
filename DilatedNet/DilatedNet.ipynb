{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DilatedNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8CMmRYwN2KNA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv_relu(in_ch,out_ch,size=3,rate=1):\n",
        "  conv_relu = nn.Sequential(nn.Conv2d(in_channels=in_ch,\n",
        "                                      out_channels=out_ch,\n",
        "                                      kernel_size=size,\n",
        "                                      stride=1,\n",
        "                                      padding=rate,\n",
        "                                      dilation=rate),\n",
        "                            nn.ReLU())\n",
        "  return conv_relu\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(VGG16,self).__init__()\n",
        "    self.features1 = nn.Sequential(conv_relu(3, 64, 3, 1),\n",
        "                                    conv_relu(64, 64, 3, 1),\n",
        "                                    nn.MaxPool2d(2, stride=2, padding=0)) # 1/2 기존 MaxPooling \n",
        "    self.features2 = nn.Sequential(conv_relu(64, 128, 3, 1),\n",
        "                                    conv_relu(128, 128, 3, 1),\n",
        "                                    nn.MaxPool2d(2, stride=2, padding=0)) # 1/2\n",
        "    self.features3 = nn.Sequential(conv_relu(128, 256, 3, 1),\n",
        "                                    conv_relu(256, 256, 3, 1),\n",
        "                                    conv_relu(256, 256, 3, 1),\n",
        "                                    nn.MaxPool2d(2, stride=2, padding=0)) # 1/2\n",
        "    self.features4 = nn.Sequential(conv_relu(256, 512, 3, 1),\n",
        "                                    conv_relu(512, 512, 3, 1),\n",
        "                                    conv_relu(512, 512, 3, 1))\n",
        "    self.features5 = nn.Sequential(conv_relu(512, 512, 3, rate=2),\n",
        "                                    conv_relu(512, 512, 3, rate=2),\n",
        "                                    conv_relu(512, 512, 3, rate=2))    \n",
        "  def forward(self, x):\n",
        "      out = self.features1(x)\n",
        "      out = self.features2(out)\n",
        "      out = self.features3(out)\n",
        "      out = self.features4(out)\n",
        "      out = self.features5(out)\n",
        "      return out\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self,num_classes):\n",
        "    super(Classifier,self).__init__()\n",
        "    self.classifier = nn.Sequential(nn.Conv2d(512, 4096, kernel_size=7, dilation=4, padding=12), # 7x7 dilation 4\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout2d(0.5),\n",
        "                                    nn.Conv2d(4096, 4096, kernel_size=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.Dropout2d(0.5),\n",
        "                                    nn.Conv2d(4096, num_classes, kernel_size=1)\n",
        "                                    )\n",
        "\n",
        "  def forward(self, x): \n",
        "      out = self.classifier(x)\n",
        "      return out\n",
        "\n",
        "class BasicContextModule(nn.Module):\n",
        "  def __init__(self, num_classes):\n",
        "      super(BasicContextModule, self).__init__()\n",
        "      \n",
        "      self.layer1 = nn.Sequential(conv_relu(num_classes, num_classes, 3, 1))\n",
        "      self.layer2 = nn.Sequential(conv_relu(num_classes, num_classes, 3, 1))\n",
        "      self.layer3 = nn.Sequential(conv_relu(num_classes, num_classes, 3, 2))\n",
        "      self.layer4 = nn.Sequential(conv_relu(num_classes, num_classes, 3, 4))\n",
        "      self.layer5 = nn.Sequential(conv_relu(num_classes, num_classes, 3, 8))\n",
        "      self.layer6 = nn.Sequential(conv_relu(num_classes, num_classes, 3, 16))\n",
        "      self.layer7 = nn.Sequential(conv_relu(num_classes, num_classes, 3, 1))\n",
        "      # No Truncation \n",
        "      self.layer8 = nn.Sequential(nn.Conv2d(num_classes, num_classes, 1, 1))\n",
        "      \n",
        "  def forward(self, x): \n",
        "      \n",
        "      out = self.layer1(x)\n",
        "      out = self.layer2(out)\n",
        "      out = self.layer3(out)\n",
        "      out = self.layer4(out)\n",
        "      out = self.layer5(out)\n",
        "      out = self.layer6(out)\n",
        "      out = self.layer7(out)\n",
        "      out = self.layer8(out)\n",
        "      \n",
        "      return out\n",
        "\n",
        "class DilatedNet(nn.Module):\n",
        "  def __init__(self, backbone, classifier, context_module):\n",
        "      super(DilatedNet, self).__init__()\n",
        "      self.backbone = backbone\n",
        "      self.classifier = classifier\n",
        "      self.context_module = context_module\n",
        "      \n",
        "      self.deconv = nn.ConvTranspose2d(in_channels=12,\n",
        "                                        out_channels=12,\n",
        "                                        kernel_size=16,\n",
        "                                        stride=8,\n",
        "                                        padding=4) # 8배\n",
        "      \n",
        "  def forward(self, x):\n",
        "      x = self.backbone(x)\n",
        "      x = self.classifier(x)\n",
        "      x = self.context_module(x)\n",
        "      out = self.deconv(x)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장\n",
        "\n",
        "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
        "backbone = VGG16()\n",
        "classifier = Classifier(num_classes=12)\n",
        "context_module = BasicContextModule(num_classes=12)\n",
        "model = DilatedNet(backbone=backbone, classifier=classifier, context_module=context_module)\n",
        "\n",
        "x = torch.randn([1, 3, 512, 512])\n",
        "print(\"input shape : \", x.shape)\n",
        "out = model(x).to(device)\n",
        "print(\"output shape : \", out.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVZ1xd70GeO0",
        "outputId": "065de013-49b6-4012-a763-2435980b2a69"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape :  torch.Size([1, 3, 512, 512])\n",
            "output shape :  torch.Size([1, 12, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xt8lf500Gr5R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}