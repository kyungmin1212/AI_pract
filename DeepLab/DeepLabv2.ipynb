{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLabv2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "87Ww0No2Lboi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def conv3x3_relu(in_ch, out_ch, rate=1):\n",
        "    conv3x3_relu = nn.Sequential(nn.Conv2d(in_ch, out_ch, kernel_size=3, \n",
        "                                    stride=1, padding=rate, dilation=rate),\n",
        "                                 nn.ReLU())\n",
        "    return conv3x3_relu\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16, self).__init__()\n",
        "        self.features = nn.Sequential(conv3x3_relu(3, 64),\n",
        "                                      conv3x3_relu(64, 64),\n",
        "                                      nn.MaxPool2d(3, stride=2, padding=1), # 1/2\n",
        "                                      conv3x3_relu(64, 128),\n",
        "                                      conv3x3_relu(128, 128),\n",
        "                                      nn.MaxPool2d(3, stride=2, padding=1), # 1/2\n",
        "                                      conv3x3_relu(128, 256),\n",
        "                                      conv3x3_relu(256, 256),\n",
        "                                      conv3x3_relu(256, 256),\n",
        "                                      nn.MaxPool2d(3, stride=2, padding=1), # 1/2\n",
        "                                      conv3x3_relu(256, 512),\n",
        "                                      conv3x3_relu(512, 512),\n",
        "                                      conv3x3_relu(512, 512),\n",
        "                                      nn.MaxPool2d(3, stride=1, padding=1), # 마지막 stride=1로 해서 두 layer 크기 유지 \n",
        "                                      # and replace subsequent conv layer r=2\n",
        "                                      conv3x3_relu(512, 512, rate=2),\n",
        "                                      conv3x3_relu(512, 512, rate=2),\n",
        "                                      conv3x3_relu(512, 512, rate=2),\n",
        "                                      nn.MaxPool2d(3, stride=1, padding=1)) # 마지막 stride=1로 해서 두 layer 크기 유지 \n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        return out\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels=1024, num_classes=21):\n",
        "        super(ASPP, self).__init__()\n",
        "        # atrous 3x3, rate=6\n",
        "        self.conv_3x3_r6 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=6, dilation=6)\n",
        "        # atrous 3x3, rate=12\n",
        "        self.conv_3x3_r12 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=12, dilation=12)\n",
        "        # atrous 3x3, rate=18\n",
        "        self.conv_3x3_r18 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=18, dilation=18)\n",
        "        # atrous 3x3, rate=24\n",
        "        self.conv_3x3_r24 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=24, dilation=24)\n",
        "        self.drop_conv_3x3 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.conv_1x1 = nn.Conv2d(out_channels, out_channels, kernel_size=1)\n",
        "        self.drop_conv_1x1 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.conv_1x1_out = nn.Conv2d(out_channels, num_classes, kernel_size=1)\n",
        "\n",
        "    def forward(self, feature_map):\n",
        "        # feature_map을 각각의 branch에 따로따로 넣어줌 그리고 최종적으로 sum한 값을 return\n",
        "\n",
        "        # 1번 branch\n",
        "        # shape: (batch_size, out_channels, height/output_stride, width/output_stride)\n",
        "        out_3x3_r6 = self.drop_conv_3x3(F.relu(self.conv_3x3_r6(feature_map))) \n",
        "        out_img_r6 = self.drop_conv_1x1(F.relu(self.conv_1x1(out_3x3_r6)))\n",
        "        out_img_r6 = self.conv_1x1_out(out_img_r6)\n",
        "        # 2번 branch\n",
        "        # shape: (batch_size, out_channels, height/output_stride, width/output_stride)\n",
        "        out_3x3_r12 = self.drop_conv_3x3(F.relu(self.conv_3x3_r12(feature_map)))\n",
        "        out_img_r12 = self.drop_conv_1x1(F.relu(self.conv_1x1(out_3x3_r12)))\n",
        "        out_img_r12 = self.conv_1x1_out(out_img_r12)\n",
        "        # 3번 branch\n",
        "        # shape: (batch_size, out_channels, height/output_stride, width/output_stride)\n",
        "        out_3x3_r18 = self.drop_conv_3x3(F.relu(self.conv_3x3_r18(feature_map)))\n",
        "        out_img_r18 = self.drop_conv_1x1(F.relu(self.conv_1x1(out_3x3_r18)))\n",
        "        out_img_r18 = self.conv_1x1_out(out_img_r18)\n",
        "        # 4번 branch\n",
        "        # shape: (batch_size, out_channels, height/output_stride, width/output_stride)\n",
        "        out_3x3_r24 = self.drop_conv_3x3(F.relu(self.conv_3x3_r24(feature_map)))\n",
        "        out_img_r24 = self.drop_conv_1x1(F.relu(self.conv_1x1(out_3x3_r24)))\n",
        "        out_img_r24 = self.conv_1x1_out(out_img_r24)\n",
        "\n",
        "        out = sum([out_img_r6, out_img_r12, out_img_r18, out_img_r24])\n",
        "        \n",
        "        return out\n",
        "\n",
        "class DeepLabV2(nn.Module):\n",
        "    ## VGG 위에 ASPP 쌓기\n",
        "    def __init__(self, backbone, classifier, upsampling=8):\n",
        "        super(DeepLabV2, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.classifier = classifier\n",
        "        self.upsampling = upsampling\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x) # 1/8\n",
        "        _, _, feature_map_h, feature_map_w = x.size()\n",
        "        x = self.classifier(x)\n",
        "        out = F.interpolate(x, size=(feature_map_h * self.upsampling, feature_map_w * self.upsampling), mode=\"bilinear\") # 8배\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장\n",
        "\n",
        "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
        "backbone = VGG16()\n",
        "aspp_module = ASPP(in_channels=512, out_channels=256, num_classes=12)\n",
        "model = DeepLabV2(backbone=backbone, classifier=aspp_module)\n",
        "\n",
        "x = torch.randn([1, 3, 512, 512])\n",
        "print(\"input shape : \", x.shape)\n",
        "out = model(x).to(device)\n",
        "print(\"output shape : \", out.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSpKcREdNaKb",
        "outputId": "12d45aee-5aab-4c78-89e0-e9db0ab5ce57"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape :  torch.Size([1, 3, 512, 512])\n",
            "output shape :  torch.Size([1, 12, 512, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jFNuKKg3NgpV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}